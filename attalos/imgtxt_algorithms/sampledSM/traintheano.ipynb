{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import attalos.imgtxt_algorithms.util.negsamp as negsamp\n",
    "import sys\n",
    "import matplotlib.pylab as plt\n",
    "from progressbar import ProgressBar as pb \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "d = 100\n",
    "f = 4096\n",
    "hidden = 4096\n",
    "V = 291\n",
    "m=10\n",
    "\n",
    "numlayers = 1\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "batchsize=256\n",
    "weightfile = None # 'params-2layer.npz'\n",
    "pretrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('data/iaprtc_alexfc7.npz')\n",
    "D = open('data/iaprtc_dictionary.txt').read().splitlines()\n",
    "train_ims = [ im.split('/')[-1] for im in open('data/iaprtc_trainlist.txt').read().splitlines() ]\n",
    "test_ims = [ im.split('/')[-1] for im in open('data/iaprtc_testlist.txt').read().splitlines() ]\n",
    "xTr = data['xTr'].T\n",
    "yTr = data['yTr'].T\n",
    "xTe = data['xTe'].T\n",
    "yTe = data['yTe'].T\n",
    "wc = yTr.sum(axis=0)+0.01-0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if weightfile and pretrain:\n",
    "    Wi = theano.shared(np.load(weightfile)['Wi'])\n",
    "    Wh = theano.shared(np.random.ranf((hidden, f)))\n",
    "    Wc = theano.shared(np.load(weightfile)['Wc'])\n",
    "elif weightfile:\n",
    "    Wi = theano.shared(np.load(weightfile)['Wi'])\n",
    "    Wh = theano.shared(np.load(weightfile)['Wh'])\n",
    "    Wc = theano.shared(np.load(weightfile)['Wc'])\n",
    "else:\n",
    "    Wh = theano.shared(np.random.ranf((hidden, f)))\n",
    "    Wi = theano.shared(np.random.ranf((d, hidden)))\n",
    "    Wc = theano.shared(np.random.ranf((V, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Negative sampler\n",
    "ns = negsamp.NegativeSampler(wc / wc.sum())\n",
    "\n",
    "# Define functionality\n",
    "x = T.matrix()\n",
    "p = T.matrix()\n",
    "n = T.matrix()\n",
    "\n",
    "# Cross correlation\n",
    "if numlayers==1:\n",
    "    xcorr = Wc.dot(Wi.dot(x.T))\n",
    "else:\n",
    "    xcorr = Wc.dot(Wi.dot(T.nnet.relu(Wh.dot(x.T))))\n",
    "\n",
    "# Intuitive that these are the same\n",
    "loss = -T.log(T.nnet.sigmoid(p.dot(xcorr))).mean() - T.log(T.nnet.sigmoid(-n.dot(xcorr))).mean()\n",
    "# loss = -T.log(T.nnet.sigmoid( (p-n).dot(xcorr)  )).mean()\n",
    "\n",
    "# Define the gradient updates. Use positive for maximization\n",
    "if numlayers==1:\n",
    "    params = [Wi, Wc]\n",
    "    gWi, gWc = T.grad(loss, params)\n",
    "    sgd = OrderedDict( { Wi: Wi - lr*gWi, Wc: Wc - lr*gWc } )\n",
    "else:\n",
    "    params = [Wi, Wc, Wh]\n",
    "    gWi, gWc, gWh = T.grad(loss, params)\n",
    "    sgd = OrderedDict( { Wi: Wi - lr*gWi, Wc: Wc - lr*gWc, Wh: Wh - lr*gWh } )\n",
    "\n",
    "# Compile to theano functionality\n",
    "train = theano.function( [x,p,n], outputs=loss, updates=sgd, allow_input_downcast=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pb(len(yTr))\n",
    "\n",
    "# Iterate through the data size\n",
    "for j in xrange(epochs):\n",
    "    print \"Epoch \"+str(j)\n",
    "    k=0\n",
    "    totloss = 0.0\n",
    "    batloss = 0.0\n",
    "    randorder = np.random.permutation(len(yTr))\n",
    "    for i in range(0,len(randorder),batchsize):\n",
    "        \n",
    "        indata = xTr[i:i+batchsize]\n",
    "        outdata= yTr[i:i+batchsize]\n",
    "\n",
    "        lossval = train( indata, outdata, ns.negsampv(outdata, m) )\n",
    "        totloss += lossval\n",
    "        batloss += lossval\n",
    "\n",
    "        k+=1\n",
    "        if k % 16 == 0:\n",
    "            p.animate(k*batchsize)\n",
    "            print('\\nlosses (inst, bat, tot)=({},{},{})'.format(lossval, batloss, totloss))\n",
    "            \n",
    "    print \"\"\n",
    "    print \"Total loss on epoch \"+str(j)+\" = \"+str(totloss)\n",
    "    \n",
    "    if numlayers==1:\n",
    "        np.savez('params-ix.npz', Wi=Wi.get_value(), Wc=Wc.get_value(), Epoch=j)      \n",
    "    else:\n",
    "        np.savez('params-ix.npz', Wi=Wi.get_value(), Wh=Wh.get_value(), Wc=Wc.get_value(), Epoch=j)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if numlayers==1:\n",
    "    np.savez('params-ix.npz', Wi=Wi.get_value(), Wc=Wc.get_value(), Epoch=j)      \n",
    "else:\n",
    "    np.savez('params-ix.npz', Wi=Wi.get_value(), Wh=Wh.get_value(), Wc=Wc.get_value(), Epoch=j)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
