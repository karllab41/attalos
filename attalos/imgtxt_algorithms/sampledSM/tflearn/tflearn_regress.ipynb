{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# from attalos.dataset.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('data/iaprtc_alexfc7.npz')\n",
    "D = open('data/iaprtc_dictionary.txt').read().splitlines()\n",
    "train_ims = [ im.split('/')[-1] for im in open('data/iaprtc_trainlist.txt').read().splitlines() ]\n",
    "test_ims = [ im.split('/')[-1] for im in open('data/iaprtc_testlist.txt').read().splitlines() ]\n",
    "xTr = data['xTr'].T\n",
    "yTr = data['yTr'].T\n",
    "xTe = data['xTe'].T\n",
    "yTe = data['yTe'].T\n",
    "wc = yTr.sum(axis=0)+0.01-0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tags_2_vec(tags, w2v_model=None):\n",
    "    if len(tags) == 0:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        return w2v_model[tags[0]]  # TODO: Only taking first tag right now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset, test_dataset, w2v_model, batch_size=128):\n",
    "    # Get a single batch to allow us to get feature vector sizes\n",
    "    image_feats, text_tags = train_dataset.get_next_batch(5)\n",
    "    word_feats = [tags_2_vec(tags, w2v_model) for tags in text_tags]\n",
    "    img_feat_size = image_feats.shape[1]\n",
    "    w2v_feat_size = word_feats[0].shape[0]\n",
    "\n",
    "    num_items = train_dataset.num_images  # TODO: Don't direct access member variables\n",
    "\n",
    "    # Allocate GPU memory as needed (vs. allocating all the memory)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    with tf.Graph().as_default():\n",
    "        # Placeholders for data\n",
    "        X = tf.placeholder(shape=(None, img_feat_size), dtype=tf.float32)\n",
    "        Y = tf.placeholder(shape=(None, w2v_feat_size), dtype=tf.float32)\n",
    "\n",
    "        # Two layer network\n",
    "        fc1 = tf.contrib.layers.fully_connected(X, 300, tf.nn.relu)\n",
    "        fc2 = tf.contrib.layers.fully_connected(fc1, 300, tf.sigmoid)\n",
    "\n",
    "        # Mean Square error\n",
    "        loss = tf.reduce_sum(tf.square(Y-fc2))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init)\n",
    "            for epoch in range(200):\n",
    "                for batch in range(int(num_items/batch_size)):\n",
    "                    image_feats, text_tags = get_next_batch(batch_size)\n",
    "                    word_feats = [tags_2_vec(tags, w2v_model) for tags in text_tags]\n",
    "                    sess.run(optimizer, feed_dict={X: image_feats, Y: word_feats})\n",
    "                cost = sess.run(loss, feed_dict={X: image_feats, Y: word_feats})\n",
    "                print('Epoch:', epoch, 'Cost:', cost)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Two layer linear regression')\n",
    "    parser.add_argument('--image_feature_file',\n",
    "                      dest='image_feature_file',\n",
    "                      type=str,\n",
    "                      help='Image Feature file (train)')\n",
    "    parser.add_argument('--text_feature_file',\n",
    "                      dest='text_feature_file',\n",
    "                      type=str,\n",
    "                      help='Text Feature file')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train_dataset = Dataset(args.image_feature_file, args.text_feature_file)\n",
    "\n",
    "    dataset_tags = set()\n",
    "    for tags in train_dataset.text_feats.values():\n",
    "        dataset_tags.update(tags)\n",
    "\n",
    "    # Read w2vec\n",
    "    w2v_lookup = {}\n",
    "    for i, line in enumerate(gzip.open('data/wiki-glove/glove.6B.200d.txt')):\n",
    "        first_word = line[:line.find(' ')]\n",
    "        if first_word in dataset_tags:\n",
    "            line = line.strip().split(' ')\n",
    "            w2v_lookup[line[0]] = np.array([ float(j) for j in line[1:]])\n",
    "\n",
    "    train_model(train_dataset, None, w2v_lookup)  # TODO: Also read testing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
