{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import readw2v as rw2v\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = rw2v.ReadW2V('wordvecs/text9Bvin.bin')\n",
    "vectors = F.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('data/iaprtc_alexfc7.npz')\n",
    "D = open('data/iaprtc_dictionary.txt').read().splitlines()\n",
    "train_ims = [ im.split('/')[-1] for im in open('data/iaprtc_trainlist.txt').read().splitlines() ]\n",
    "test_ims = [ im.split('/')[-1] for im in open('data/iaprtc_testlist.txt').read().splitlines() ]\n",
    "xTr = data['xTr'].T\n",
    "yTr = data['yTr'].T\n",
    "xTe = data['xTe'].T\n",
    "yTe = data['yTe'].T\n",
    "wc = yTr.sum(axis=0)+0.00\n",
    "\n",
    "imfeatsize = xTr.shape[1]\n",
    "vocabsize = yTr.shape[1]\n",
    "wordvecsize = vectors['</s>'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "HIDDEN_SIZE = 300\n",
    "RATE = 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### If you wanted to collect them into a matrix similar to IAPR-TC12, then this would be how you would do it.\n",
    "\n",
    "matval = np.array(vectors.values())\n",
    "words = vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetwords = [ D[i] for i in np.nonzero( yTr[np.random.randint(len(yTr))] )[0] ]\n",
    "targetvecs = np.array( [vectors[w] for w in targetwords ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Things not in the Wikipedia right now:\n",
    "\n",
    "'tee-shirt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a single feature, get the related training vectors\n",
    "\n",
    "Usage:  (im, veclist) = get_training_vectors( index_number )\n",
    "\n",
    "### For multiple features (in the list of indices), for each image feature, sum the training vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given an index into the corpus, get the corresponding image feature and all word vectors from it\n",
    "def get_training_vectors(index):\n",
    "    yTrList = []\n",
    "    for i in np.nonzero( yTr[index] )[0]:\n",
    "        yTrList.append( vectors[ D[i] ] )\n",
    "    return (xTr[index], np.array(yTrList))\n",
    "\n",
    "def get_training_batch(indices):\n",
    "    \n",
    "    feats = np.zeros((len(indices),imfeatsize))\n",
    "    labels = np.zeros((len(indices),wordvecsize))\n",
    "    \n",
    "    for j, i in enumerate(indices):\n",
    "        feats[j], wordfeats = get_training_vectors(i)\n",
    "        labels[j] = wordfeats.sum(axis=0)\n",
    "    return (feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feature size = 4096\n",
      "Image vocabulary size = 291\n",
      "Looking to extend that to 218317\n",
      "Initializing tensorflow variables\n"
     ]
    }
   ],
   "source": [
    "print \"Image feature size = {}\".format( imfeatsize )\n",
    "print \"Image vocabulary size = {}\".format( vocabsize )\n",
    "print \"Looking to extend that to {}\".format( len(vectors) )\n",
    "\n",
    "print \"Initializing tensorflow variables\"\n",
    "# Tensorflow input output data\n",
    "X = tf.placeholder(tf.float32, shape=[None, imfeatsize], name='Input')\n",
    "Ytruth = tf.placeholder(tf.float32, shape=[None, wordvecsize], name='Truth')\n",
    "# positive_examples = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HIDDEN_SIZE], name=\"Positive_Examples\")\n",
    "# negative_examples = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HIDDEN_SIZE], name=\"Negative_Examples\")\n",
    "\n",
    "# Tensorflow variable weights\n",
    "W0 = tf.Variable(tf.random_normal([imfeatsize, HIDDEN_SIZE],stddev=1))\n",
    "W1 = tf.Variable(tf.random_normal([HIDDEN_SIZE, wordvecsize], stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two layer perceptron\n",
    "def two_layer_perceptron(X_i, Wt0, Wt1):\n",
    "    hidden_layer = tf.nn.sigmoid(tf.matmul(X_i, Wt0))\n",
    "    return tf.nn.sigmoid( tf.matmul(hidden_layer, Wt1) )\n",
    "Y = two_layer_perceptron(X, W0, W1)\n",
    "\n",
    "binxentropy = tf.reduce_mean(-tf.reduce_sum(Ytruth * tf.log(Y) + (1 - Ytruth)*tf.log(Y), reduction_indices=[1]))\n",
    "\n",
    "\n",
    "# Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=RATE).minimize(binxentropy)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=RATE).minimize(tf.nn.sigmoid_cross_entropy_with_logits)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=RATE).minimize(binxentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0Prior: [[ 0.21997467 -1.19393682 -0.18705678 ...,  0.42633942 -0.07044954\n",
      "  -0.56638432]\n",
      " [-0.71226329  0.89928865 -0.83504802 ...,  0.51589924  1.18704534\n",
      "   0.80860144]\n",
      " [-0.42435294 -0.85224849  1.41782522 ...,  0.37382987  1.4340049\n",
      "  -0.19368374]\n",
      " ..., \n",
      " [ 1.25981188 -0.78272468  1.41784775 ..., -0.24662268  0.55633718\n",
      "  -0.8620255 ]\n",
      " [-1.62061107 -0.08030051  1.69193399 ...,  1.98095596  1.27665401\n",
      "   1.27177298]\n",
      " [ 0.63576293  0.40372279  1.95382655 ..., -1.42940867 -1.50172448\n",
      "   1.18037844]]\n",
      "W0SessRun: [[ 100.2197876    98.80587769   99.8127594  ...,  100.42602539\n",
      "    99.92936707   99.43343353]\n",
      " [  99.28760529  100.89924622   99.16490173 ...,  100.51585388\n",
      "   101.18699646  100.80853271]\n",
      " [  99.57553101   99.14768982  101.41777039 ...,  100.37376404\n",
      "   101.43395233   99.80625153]\n",
      " ..., \n",
      " [ 101.25971985   99.21724701  101.41781616 ...,   99.75333405\n",
      "   100.55630493   99.13793182]\n",
      " [  98.37915802   99.91962433  101.69186401 ...,  101.98088074\n",
      "   101.27657318  101.27165985]\n",
      " [ 100.63567352  100.40366364  101.95376587 ...,   98.57052612\n",
      "    98.49821472  101.18032837]]\n",
      "W0OptRun: [[ 167.22525024  165.81134033  166.8182373  ...,  167.43141174\n",
      "   166.93484497  166.43890381]\n",
      " [ 166.29310608  167.90481567  166.17047119 ...,  167.52142334\n",
      "   168.19256592  167.81408691]\n",
      " [ 166.58105469  166.15324402  168.42332458 ...,  167.37930298\n",
      "   168.43951416  166.8117981 ]\n",
      " ..., \n",
      " [ 168.26524353  166.22280884  168.42338562 ...,  166.75889587\n",
      "   167.56188965  166.14349365]\n",
      " [ 165.38459778  166.9251709   168.69741821 ...,  168.98641968\n",
      "   168.28210449  168.2771759 ]\n",
      " [ 167.64120483  167.40921021  168.95932007 ...,  165.57608032\n",
      "   165.50376892  168.18588257]]\n",
      "None\n",
      "905.562\n",
      "[[  3.90190951e-04   8.38212203e-03   9.40798691e-06 ...,   2.99239531e-04\n",
      "    6.16411002e-13   3.94140045e-07]\n",
      " [  9.99981523e-01   3.62753926e-05   3.12749557e-06 ...,   3.03865243e-02\n",
      "    5.32317195e-08   5.81150573e-07]\n",
      " [  2.20191196e-01   3.78191487e-08   6.09606587e-08 ...,   2.62872391e-09\n",
      "    9.65124741e-07   1.05855048e-01]\n",
      " [  9.98931587e-01   4.95614131e-06   2.74551706e-08 ...,   9.83367682e-01\n",
      "    7.00589666e-08   2.49205214e-12]\n",
      " [  1.73498746e-02   3.28764529e-03   7.09184818e-08 ...,   4.00355607e-01\n",
      "    1.57817176e-10   1.36522671e-09]\n",
      " [  9.88885403e-01   4.99682259e-16   5.34896462e-05 ...,   2.79533342e-02\n",
      "    5.34546096e-09   7.16121793e-01]]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # sess = tf.InteractiveSession()\n",
    "    \n",
    "    # Initialize the variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print \"W0Prior: {}\".format(W1.eval())\n",
    "    \n",
    "    # Get a batch for training\n",
    "    imbatch, wordbatch = get_training_batch([0,1,2,3,4,5])\n",
    "    indict={X:imbatch, Ytruth:wordbatch}\n",
    "    \n",
    "    # Run the session\n",
    "    optval, lossval, yval = sess.run([optimizer, binxentropy, Y], feed_dict=indict)\n",
    "    print \"W0SessRun: {}\".format(W1.eval())\n",
    "    \n",
    "    # Separately run the optimizer\n",
    "    optimizer.run(feed_dict=indict)\n",
    "    print \"W0OptRun: {}\".format(W1.eval())\n",
    "    \n",
    "    print optval\n",
    "    print lossval\n",
    "    print yval\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
