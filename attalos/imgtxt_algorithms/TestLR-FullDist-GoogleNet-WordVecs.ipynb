{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Linear Regression\n",
    "\n",
    "This notebook is an example that will test the generalization capability of a regression to word vectors. There are three corpora involved.\n",
    "\n",
    "## Required Data\n",
    "\n",
    "1. The _word vector corpora_\n",
    "  * Examples: New York Times, Wikipedia Text8\n",
    "  * Data: Pretrained word vectors (word2vec, etc.)\n",
    "2. The _training corpora_\n",
    "  * Examples: IAPR-TC12, MSCOCO, Visual Genome\n",
    "  * Data:\n",
    "    - Training image features and text labels\n",
    "    - Testing image features and text labels <-- Used as validation data\n",
    "3. A _testing corpora_ with a different vocabulary\n",
    "  * Examples: MSCOCO, Visual Genome, etc.\n",
    "  * Data: Training and testing image features\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "from six.moves import reload_module as reload\n",
    "\n",
    "## Should probably update to PYTHONPATH\n",
    "sys.path.append('/data/fs4/home/kni/attalos/')\n",
    "# ls /data/fs4/home/kni/attalos/\n",
    "from attalos.dataset.dataset import Dataset\n",
    "imdir = '/data/fs4/teams/attalos/features/image/'\n",
    "txdir = '/data/fs4/teams/attalos/features/text/'\n",
    "\n",
    "## Import word vector load in\n",
    "import attalos.imgtxt_algorithms.util.readw2v as rw2v\n",
    "\n",
    "## Import linear regression\n",
    "import attalos.imgtxt_algorithms.linearregression.LinearRegression as linreg\n",
    "\n",
    "## Import evaluation code (right now, using Octave soft evaluation)\n",
    "# from attalos.evaluation.evaluation import Eval\n",
    "from oct2py import octave\n",
    "octave.addpath('../evaluation/')\n",
    "\n",
    "reload(linreg)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word vectors in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXSTRING=100\n",
    "\n",
    "def readstring(fd):\n",
    "\n",
    "    word=''\n",
    "\n",
    "    for i in range(MAXSTRING):\n",
    "        char = fd.read(1)\n",
    "        if char.isspace():\n",
    "            break\n",
    "        else:\n",
    "            word+=char.decode('utf-8')\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(rw2v)\n",
    "F = rw2v.ReadW2V('/data/fs4/teams/attalos/wordvecs/text9Bvin.bin')\n",
    "vectors = F.readlines()\n",
    "\n",
    "# f = open('/data/fs4/teams/attalos/wordvecs/text9Bvin.bin','rb')\n",
    "# print('{}'.format( readstring(f) ) )\n",
    "# print('{}'.format( readstring(f) ) )\n",
    "# byte = f.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training corpora in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, not 'bytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fa5dd8aa6e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'iaprtc12_train_inception.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'iaprtc12_train_text.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'iaprtc12_test_inception.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'iaprtc12_test_text.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fs4/home/kni/attalos/attalos/dataset/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_feature_filename, text_feature_filename, text_feat_type, tag_transfomer, seed)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_transfomer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_text_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__load_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fs4/home/kni/attalos/attalos/dataset/dataset.py\u001b[0m in \u001b[0;36m__load_text_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_feature_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_feat_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.4/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.4/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         raise TypeError('the JSON object must be str, not {!r}'.format(\n\u001b[0;32m--> 312\u001b[0;31m                             s.__class__.__name__))\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'\\ufeff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected UTF-8 BOM (decode using utf-8-sig)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, not 'bytes'"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(imdir+'iaprtc12_train_inception.hdf5', txdir+'iaprtc12_train_text.json.gz')\n",
    "test_dataset = Dataset(imdir+'iaprtc12_test_inception.hdf5', txdir+'iaprtc12_test_text.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = open('linearregression/data/iaprtc_dictionary.txt').read().splitlines()\n",
    "reverse_D = dict()\n",
    "for i in range(len(D)): reverse_D[D[i]] = i \n",
    "    \n",
    "xTr, yTrWds = train_dataset.get_next_batch(17565)\n",
    "xTe, yTeWds = test_dataset.get_next_batch(1962)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('linearregression/data/iaprtc_alexfc7.npz')\n",
    "train_ims = [ im.split('/')[-1] for im in open('linearregression/data/iaprtc_trainlist.txt').read().splitlines() ]\n",
    "\n",
    "test_ims_full = [ im for im in open('linearregression/data/iaprtc_testlist.txt').read().splitlines() ]\n",
    "train_ims_full = [ im for im in open('linearregression/data/iaprtc_trainlist.txt').read().splitlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create probability distribution for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvecs = np.zeros((len(D), 200))\n",
    "for i, word in enumerate(D):\n",
    "    if vectors.has_key( word ):\n",
    "        wordvecs[i] = vectors[word]\n",
    "    else:\n",
    "        print \"{}: {}\".format(word,i)\n",
    "        \n",
    "distvecs = 1 / (1 + np.exp( - 0.1*wordvecs.dot(wordvecs.T) ) )\n",
    "# distvecs = np.tanh( 0.1*wordvecs.dot(wordvecs.T) )\n",
    "distvecs = distvecs / np.linalg.norm( distvecs, axis=1 )\n",
    "\n",
    "wdvecs=dict()\n",
    "for i, word in enumerate(D):\n",
    "    wdvecs[word] = distvecs[i]\n",
    "    \n",
    "plt.stem( wdvecs['shore'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert multi-hot vectors to distribution vectors\n",
    "\n",
    "1. wordvecs -> distvecs\n",
    "2. onehot -> distvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all images in the training set to distribution vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTr = np.zeros( (len(yTrWds), len(D)) )\n",
    "for i, imtaglist in enumerate(yTrWds):\n",
    "    for tag in imtaglist:\n",
    "        # if tag in ['tee-shirt','bedcover','table-cloth']:\n",
    "        #     continue\n",
    "        yTr[i]+=wdvecs[tag]\n",
    "    if yTr[i].sum() == 0:\n",
    "        print \"vector \"+str(i)+\" has no tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTe = np.zeros( (len(yTeWds), len(D)) )\n",
    "for i, imtaglist in enumerate(yTeWds):\n",
    "    for tag in imtaglist:\n",
    "        yTe[i,reverse_D[tag]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing corpora in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(imdir+'espgame-inria-train.hdf5', txdir+'espgame_text_train.json.gz')\n",
    "test_dataset = Dataset(imdir+'espgame-inria-test.hdf5', txdir+'espgame_text_test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print xTr.shape\n",
    "print yTr.shape\n",
    "mp_solution = linreg.LinearRegression(normX = True, normY = True)\n",
    "mp_solution.train(xTr, yTr)\n",
    "yHat = mp_solution.predict(xTe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[precision, recall, f1score] = octave.evaluate(yTe.T, yHat.T, 5)\n",
    "print precision\n",
    "print recall\n",
    "print f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly select an image\n",
    "i=np.random.randint(0, yTe.shape[1])\n",
    "\n",
    "# Run example\n",
    "imname='linearregression/images/'+test_ims_full[i]+'.jpg';\n",
    "print \"Looking at the \"+str(i)+\"th image: \"+imname\n",
    "im=plt.imread(imname)\n",
    "\n",
    "# Prediction\n",
    "ypwords = [D[j] for j in yHat[i].argsort()[::-1] [ 0:(yHat[i]>0.2).sum() ] ]\n",
    "# Truth\n",
    "ytwords = [D[j] for j in np.where(yTe[i] > 0.5)[0] ]\n",
    "\n",
    "plt.imshow(im)\n",
    "print 'Predicted: '+ ', '.join(ypwords)\n",
    "print 'Truth:     '+ ', '.join(ytwords)\n",
    "\n",
    "plt.figure()\n",
    "plt.stem( yHat[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
